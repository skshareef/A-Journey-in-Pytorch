{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNxSCH68FKLZInjJ9PUipnn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rUgcqdjIO3jU"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["# In this notebook:\n","\n","1.   prediction\n","2.   Compute the Gradients: Autograd\n","3.   Loss computation(pytorch loss)\n","4.   Parameter updates(Pytorch optimizer)\n","\n"],"metadata":{"id":"hMk7bf_1O9B1"}},{"cell_type":"code","source":["from os import X_OK\n","# assuming you know how to do linear regression\n","# doing everything manually\n","import numpy as np\n","#f = w*x\n","\n","x=np.array([1,2,3,4], dtype=np.float32)\n","y=np.array([2,4,6,8], dtype=np.float32)\n","w=0.0\n","\n","#we will follow the conevcetion of the pytorch here\n","#model prediction\n","#forward pass\n","def forward(x):\n","  return w*x\n","\n","#loss\n","#MSE\n","def loss(y,y_pred):\n","  return (((y-y_pred))**2).mean()\n","\n","#gradient\n","#caculate deravtive of mse\n","def grad(x,y,y_pred):\n","  return np.dot(2*x,y_pred-y).mean()\n","\n","print(f'prediction before training : f(5)={forward(5):.3f}')\n","\n","#training\n","leaning_rate =0.01\n","n_iters = 10\n","\n","for epochs in range(n_iters):\n","    #forward\n","    y_pred=forward(x)\n","    l=loss(y,y_pred)\n","    dw=grad(x,y,y_pred)\n","\n","    #update weights\n","    w -= leaning_rate*dw\n","    print(f'epoch {epochs+1} : w={w:.3f}, loss={l:.8f}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BemGj8DEPLvj","executionInfo":{"status":"ok","timestamp":1686702309610,"user_tz":240,"elapsed":308,"user":{"displayName":"Shareef Shaik","userId":"18290925347165335265"}},"outputId":"92c5da97-7ec4-4d21-8624-1f75b21924d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["prediction before training : f(5)=0.000\n","epoch 1 : w=1.200, loss=30.00000000\n","epoch 2 : w=1.680, loss=4.79999924\n","epoch 3 : w=1.872, loss=0.76800019\n","epoch 4 : w=1.949, loss=0.12288000\n","epoch 5 : w=1.980, loss=0.01966083\n","epoch 6 : w=1.992, loss=0.00314574\n","epoch 7 : w=1.997, loss=0.00050331\n","epoch 8 : w=1.999, loss=0.00008053\n","epoch 9 : w=1.999, loss=0.00001288\n","epoch 10 : w=2.000, loss=0.00000206\n","epoch 11 : w=2.000, loss=0.00000033\n","epoch 12 : w=2.000, loss=0.00000005\n","epoch 13 : w=2.000, loss=0.00000001\n","epoch 14 : w=2.000, loss=0.00000000\n","epoch 15 : w=2.000, loss=0.00000000\n","epoch 16 : w=2.000, loss=0.00000000\n","epoch 17 : w=2.000, loss=0.00000000\n","epoch 18 : w=2.000, loss=0.00000000\n","epoch 19 : w=2.000, loss=0.00000000\n","epoch 20 : w=2.000, loss=0.00000000\n"]}]},{"cell_type":"code","source":["print(f'prediction before training : f(5)={forward(5):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8DynJizSUCG","executionInfo":{"status":"ok","timestamp":1686702313914,"user_tz":240,"elapsed":209,"user":{"displayName":"Shareef Shaik","userId":"18290925347165335265"}},"outputId":"cb37d9ea-e869-4067-fade-a9af953d332c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["prediction before training : f(5)=10.000\n"]}]},{"cell_type":"markdown","source":["## Doing everything with pytorch\n","\n","*`playing with gradients.`*"],"metadata":{"id":"zs-gRvtoS0-u"}},{"cell_type":"code","source":["x=torch.tensor([1,2,3,4], dtype=torch.float32)\n","y=torch.tensor([2,4,6,8], dtype=torch.float32)\n","w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n","def forward(x):\n","  return w*x\n","def loss(y,y_pred):\n","  return (((y-y_pred))**2).mean()\n","def grad(x,y,y_pred):\n","  return np.dot(2*x,y_pred-y).mean()\n","print(f'prediction before training : f(5)={forward(5):.3f}')\n","#training\n","leaning_rate =0.01\n","n_iters = 20\n","\n","for epochs in range(n_iters):\n","    #forward\n","    y_pred=forward(x)\n","    l=loss(y,y_pred)\n","    #dw=grad(x,y,y_pred)\n","    l.backward()\n","    # to avoid the accumulation of the weights we need to make them zero\n","    #update weights\n","    with torch.no_grad():\n","         w -= leaning_rate*w.grad\n","\n","    #zero gradients\n","\n","    w.grad.zero_()\n","\n","    print(f'epoch {epochs+1} : w={w:.3f}, loss={l:.8f}')\n","\n","\n","\n","\n","print(f'prediction before training : f(5)={forward(5):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cI9oymCiSz1X","executionInfo":{"status":"ok","timestamp":1686702774815,"user_tz":240,"elapsed":202,"user":{"displayName":"Shareef Shaik","userId":"18290925347165335265"}},"outputId":"72b9803d-8604-4ae3-d2f9-93e6fb171ab8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["prediction before training : f(5)=0.000\n","epoch 1 : w=0.300, loss=30.00000000\n","epoch 2 : w=0.555, loss=21.67499924\n","epoch 3 : w=0.772, loss=15.66018772\n","epoch 4 : w=0.956, loss=11.31448650\n","epoch 5 : w=1.113, loss=8.17471695\n","epoch 6 : w=1.246, loss=5.90623236\n","epoch 7 : w=1.359, loss=4.26725292\n","epoch 8 : w=1.455, loss=3.08308983\n","epoch 9 : w=1.537, loss=2.22753215\n","epoch 10 : w=1.606, loss=1.60939169\n","epoch 11 : w=1.665, loss=1.16278565\n","epoch 12 : w=1.716, loss=0.84011245\n","epoch 13 : w=1.758, loss=0.60698116\n","epoch 14 : w=1.794, loss=0.43854395\n","epoch 15 : w=1.825, loss=0.31684780\n","epoch 16 : w=1.851, loss=0.22892261\n","epoch 17 : w=1.874, loss=0.16539653\n","epoch 18 : w=1.893, loss=0.11949898\n","epoch 19 : w=1.909, loss=0.08633806\n","epoch 20 : w=1.922, loss=0.06237914\n","prediction before training : f(5)=9.612\n"]}]},{"cell_type":"markdown","source":["# This time we are going compute gradinets,loss and paramaters using pytorch functions\n","\n","\n","1.   design model\n","2.   construct loss and optimizer\n","3.  Training Loops(forwards pass,backward pass and update weights)\n","\n","\n","\n"],"metadata":{"id":"EnQUXuMBwuq9"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n"],"metadata":{"id":"wLM1t-XYTbq3","executionInfo":{"status":"ok","timestamp":1686712877217,"user_tz":240,"elapsed":6,"user":{"displayName":"Shareef Shaik","userId":"18290925347165335265"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["x=torch.tensor([1,2,3,4], dtype=torch.float32)\n","y=torch.tensor([2,4,6,8], dtype=torch.float32)\n","w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n","def forward(x):\n","  return w*x\n","\n","print(f'prediction before training : f(5)={forward(5):.3f}')\n","#training\n","leaning_rate =0.01\n","n_iters = 20\n","#in-built function from torch\n","loss=nn.MSELoss()\n","#decalring a optimizer\n","optimizer=torch.optim.SGD([w], lr=leaning_rate)\n","\n","\n","for epoch in range(n_iters):\n","    y_pred=forward(x)\n","    l=loss(y,y_pred)\n","    l.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    print(f'epoch {epoch+1} : w={w:.3f}, loss={l:.8f}')\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hmlcNvVmxU16","executionInfo":{"status":"ok","timestamp":1686710585970,"user_tz":240,"elapsed":4,"user":{"displayName":"Shareef Shaik","userId":"18290925347165335265"}},"outputId":"16b82694-4a95-444c-d761-cba10190f7ca"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["prediction before training : f(5)=0.000\n","epoch 1 : w=0.300, loss=30.00000000\n","epoch 2 : w=0.555, loss=21.67499924\n","epoch 3 : w=0.772, loss=15.66018772\n","epoch 4 : w=0.956, loss=11.31448650\n","epoch 5 : w=1.113, loss=8.17471695\n","epoch 6 : w=1.246, loss=5.90623236\n","epoch 7 : w=1.359, loss=4.26725292\n","epoch 8 : w=1.455, loss=3.08308983\n","epoch 9 : w=1.537, loss=2.22753215\n","epoch 10 : w=1.606, loss=1.60939169\n","epoch 11 : w=1.665, loss=1.16278565\n","epoch 12 : w=1.716, loss=0.84011245\n","epoch 13 : w=1.758, loss=0.60698116\n","epoch 14 : w=1.794, loss=0.43854395\n","epoch 15 : w=1.825, loss=0.31684780\n","epoch 16 : w=1.851, loss=0.22892261\n","epoch 17 : w=1.874, loss=0.16539653\n","epoch 18 : w=1.893, loss=0.11949898\n","epoch 19 : w=1.909, loss=0.08633806\n","epoch 20 : w=1.922, loss=0.06237914\n"]}]},{"cell_type":"code","source":["# we are using pytorch model in this part\n","# we dont need weights\n","# Usually we need to build the model\n","# As from the statement we know it as a Linear Model, we are going to use in-built linear fucntion from pytorch\n","# Need to convert the 'x' and 'y' into arrays\n","\n","x=torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n","y=torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n","#w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n","#def forward(x):\n","#  return w*x\n","\n","\n","#declaring test varaibles\n","x_test=torch.tensor([5], dtype=torch.float32)\n","y_test=torch.tensor([10], dtype=torch.float32)\n","\n","n_samples,n_features=x.shape\n","input_size=n_features\n","output_size=n_features\n","model=nn.Linear(input_size,output_size)\n","print(f'prediction before training : f(5)={model(x_test).item():.3f}')\n","\n","#in-built function from torch\n","loss=nn.MSELoss()\n","#decalring a optimizer\n","optimizer=torch.optim.SGD(model.parameters(), lr=leaning_rate)\n","\n","\n","leaning_rate =0.02\n","n_iters = 100\n","\n","for epochs in range(n_iters):\n","    #forward\n","    y_pred=model(x)\n","    l=loss(y,y_pred)\n","    #dw=grad(x,y,y_pred)\n","    l.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    [w,b]=model.parameters()\n","\n","    print(f'epoch {epochs+1} : w={w[0][0].item():.3f}, loss={l:.8f}')\n","\n","\n","\n","\n","print(f'prediction before training : f(5)={model(x_test).item():.3f}')\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHqVKO_2ySb3","executionInfo":{"status":"ok","timestamp":1686714598823,"user_tz":240,"elapsed":11,"user":{"displayName":"Shareef Shaik","userId":"18290925347165335265"}},"outputId":"fbcff7e1-e235-452d-db38-f02fff8363dd"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["prediction before training : f(5)=-2.379\n","epoch 1 : w=-0.291, loss=41.97760010\n","epoch 2 : w=0.004, loss=29.25065231\n","epoch 3 : w=0.250, loss=20.41896248\n","epoch 4 : w=0.456, loss=14.29010391\n","epoch 5 : w=0.627, loss=10.03669167\n","epoch 6 : w=0.770, loss=7.08461475\n","epoch 7 : w=0.890, loss=5.03551102\n","epoch 8 : w=0.990, loss=3.61296654\n","epoch 9 : w=1.073, loss=2.62518072\n","epoch 10 : w=1.143, loss=1.93907177\n","epoch 11 : w=1.201, loss=1.46229196\n","epoch 12 : w=1.250, loss=1.13076603\n","epoch 13 : w=1.291, loss=0.90003264\n","epoch 14 : w=1.325, loss=0.73924160\n","epoch 15 : w=1.354, loss=0.62698668\n","epoch 16 : w=1.378, loss=0.54841292\n","epoch 17 : w=1.399, loss=0.49321496\n","epoch 18 : w=1.416, loss=0.45424020\n","epoch 19 : w=1.431, loss=0.42652684\n","epoch 20 : w=1.443, loss=0.40663159\n","epoch 21 : w=1.454, loss=0.39216492\n","epoch 22 : w=1.463, loss=0.38146901\n","epoch 23 : w=1.471, loss=0.37339377\n","epoch 24 : w=1.477, loss=0.36714029\n","epoch 25 : w=1.483, loss=0.36215529\n","epoch 26 : w=1.488, loss=0.35805410\n","epoch 27 : w=1.493, loss=0.35457009\n","epoch 28 : w=1.497, loss=0.35151806\n","epoch 29 : w=1.500, loss=0.34876958\n","epoch 30 : w=1.504, loss=0.34623566\n","epoch 31 : w=1.507, loss=0.34385422\n","epoch 32 : w=1.509, loss=0.34158230\n","epoch 33 : w=1.512, loss=0.33938995\n","epoch 34 : w=1.514, loss=0.33725697\n","epoch 35 : w=1.516, loss=0.33516833\n","epoch 36 : w=1.518, loss=0.33311436\n","epoch 37 : w=1.520, loss=0.33108792\n","epoch 38 : w=1.522, loss=0.32908434\n","epoch 39 : w=1.524, loss=0.32710001\n","epoch 40 : w=1.525, loss=0.32513273\n","epoch 41 : w=1.527, loss=0.32318085\n","epoch 42 : w=1.529, loss=0.32124302\n","epoch 43 : w=1.530, loss=0.31931841\n","epoch 44 : w=1.532, loss=0.31740659\n","epoch 45 : w=1.533, loss=0.31550699\n","epoch 46 : w=1.535, loss=0.31361932\n","epoch 47 : w=1.536, loss=0.31174332\n","epoch 48 : w=1.538, loss=0.30987889\n","epoch 49 : w=1.539, loss=0.30802578\n","epoch 50 : w=1.541, loss=0.30618387\n","epoch 51 : w=1.542, loss=0.30435288\n","epoch 52 : w=1.543, loss=0.30253321\n","epoch 53 : w=1.545, loss=0.30072418\n","epoch 54 : w=1.546, loss=0.29892620\n","epoch 55 : w=1.548, loss=0.29713899\n","epoch 56 : w=1.549, loss=0.29536235\n","epoch 57 : w=1.550, loss=0.29359648\n","epoch 58 : w=1.552, loss=0.29184103\n","epoch 59 : w=1.553, loss=0.29009619\n","epoch 60 : w=1.554, loss=0.28836167\n","epoch 61 : w=1.556, loss=0.28663757\n","epoch 62 : w=1.557, loss=0.28492385\n","epoch 63 : w=1.558, loss=0.28322035\n","epoch 64 : w=1.560, loss=0.28152686\n","epoch 65 : w=1.561, loss=0.27984381\n","epoch 66 : w=1.562, loss=0.27817053\n","epoch 67 : w=1.564, loss=0.27650747\n","epoch 68 : w=1.565, loss=0.27485424\n","epoch 69 : w=1.566, loss=0.27321100\n","epoch 70 : w=1.568, loss=0.27157742\n","epoch 71 : w=1.569, loss=0.26995382\n","epoch 72 : w=1.570, loss=0.26833987\n","epoch 73 : w=1.571, loss=0.26673549\n","epoch 74 : w=1.573, loss=0.26514068\n","epoch 75 : w=1.574, loss=0.26355547\n","epoch 76 : w=1.575, loss=0.26197973\n","epoch 77 : w=1.577, loss=0.26041329\n","epoch 78 : w=1.578, loss=0.25885639\n","epoch 79 : w=1.579, loss=0.25730872\n","epoch 80 : w=1.580, loss=0.25577033\n","epoch 81 : w=1.582, loss=0.25424105\n","epoch 82 : w=1.583, loss=0.25272113\n","epoch 83 : w=1.584, loss=0.25121000\n","epoch 84 : w=1.585, loss=0.24970806\n","epoch 85 : w=1.587, loss=0.24821523\n","epoch 86 : w=1.588, loss=0.24673115\n","epoch 87 : w=1.589, loss=0.24525595\n","epoch 88 : w=1.590, loss=0.24378970\n","epoch 89 : w=1.592, loss=0.24233207\n","epoch 90 : w=1.593, loss=0.24088322\n","epoch 91 : w=1.594, loss=0.23944302\n","epoch 92 : w=1.595, loss=0.23801138\n","epoch 93 : w=1.596, loss=0.23658842\n","epoch 94 : w=1.598, loss=0.23517388\n","epoch 95 : w=1.599, loss=0.23376782\n","epoch 96 : w=1.600, loss=0.23237021\n","epoch 97 : w=1.601, loss=0.23098090\n","epoch 98 : w=1.602, loss=0.22959986\n","epoch 99 : w=1.604, loss=0.22822708\n","epoch 100 : w=1.605, loss=0.22686248\n","prediction before training : f(5)=9.186\n"]}]}]}